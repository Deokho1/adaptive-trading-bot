"""
ğŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ê¸°

build_datasets.pyê°€ ë§Œë“  íŒŒì¼ë“¤ì´ ì •ìƒì ì¸ ìº”ë“¤ ì‹œê³„ì—´ì¸ì§€ í™•ì¸í•˜ëŠ” QA ë‹¨ê³„ì…ë‹ˆë‹¤.

ì£¼ìš” ê²€ì‚¬ í•­ëª©:
- íƒ€ì„ìŠ¤íƒ¬í”„ ì—°ì†ì„±: ìº”ë“¤ì´ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì´ì–´ì ¸ ìˆëŠ”ì§€
- ê²°ì¸¡ê°’: open/high/low/close/volume ì¤‘ NaN ìˆëŠ”ì§€  
- ì¤‘ë³µ: ë™ì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì¤‘ë³µ ìº”ë“¤ ì¡´ì¬ ì—¬ë¶€
- ì´ìƒì¹˜: ê±°ë˜ëŸ‰ ë˜ëŠ” ê°€ê²© ê¸‰ë“±ë½ (í†µê³„ì  ê¸°ì¤€)
- ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„±: ensure_candle_schema()ë¡œ êµ¬ì¡° ê²€ì¦

íë¦„: fetch_market_data.py â†’ build_datasets.py â†’ verify_integrity.py
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import shutil
import logging

from .schema import ensure_candle_schema, validate_candle_data, dataframe_to_candles


class DataIntegrityVerifier:
    """
    ğŸ•µï¸ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ê¸°
    
    ë°ì´í„° í’ˆì§ˆì„ ì²´ê³„ì ìœ¼ë¡œ ê²€ì‚¬í•˜ê³  ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ì„ ë¶„ë¦¬ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, base_data_dir: str = "backtest_data"):
        self.base_dir = Path(base_data_dir)
        self.processed_dir = self.base_dir / "processed"
        self.metadata_dir = self.base_dir / "metadata"
        
        self._ensure_directories()
        self._setup_logging()
        
    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í„°ë¦¬ ìƒì„±"""
        pass  # ê¸°ë³¸ í´ë”ë“¤ì€ build_datasets.pyì—ì„œ ìƒì„±ë¨
            
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def verify_single_file(
        self, 
        file_path,  # Union[str, Path] 
        detailed_check: bool = True
    ) -> Dict[str, Any]:
        """
        ğŸ“Š ë‹¨ì¼ íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
        
        Args:
            file_path: ê²€ì¦í•  íŒŒì¼ ê²½ë¡œ (str ë˜ëŠ” Path)
            detailed_check: ìƒì„¸ ê²€ì‚¬ ì—¬ë¶€ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
            
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # Path ê°ì²´ë¡œ ë³€í™˜
        if isinstance(file_path, str):
            file_path = Path(file_path)
            
        self.logger.info(f"ğŸ” ê²€ì¦ ì‹œì‘: {file_path.name}")
        
        verification_result = {
            "file_path": str(file_path),
            "file_name": file_path.name,
            "verification_timestamp": datetime.now().isoformat(),
            "file_size_mb": file_path.stat().st_size / (1024 * 1024),
            "status": "unknown",
            "issues": [],
            "metrics": {},
            "recommendations": []
        }
        
        try:
            # 1. íŒŒì¼ ë¡œë”© ë° ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path, parse_dates=['timestamp'])
            elif file_path.suffix == '.parquet':
                df = pd.read_parquet(file_path)
            elif file_path.suffix == '.pkl':
                df = pd.read_pickle(file_path)
            else:
                verification_result["status"] = "unsupported_format"
                verification_result["issues"].append(f"Unsupported file format: {file_path.suffix}")
                return verification_result
            
            verification_result["metrics"]["total_rows"] = len(df)
            verification_result["metrics"]["memory_usage_mb"] = df.memory_usage(deep=True).sum() / (1024 * 1024)
            
            if df.empty:
                verification_result["status"] = "empty_file"
                verification_result["issues"].append("File is empty")
                return verification_result
            
            # 2. ìŠ¤í‚¤ë§ˆ ê²€ì¦
            try:
                df = ensure_candle_schema(df, strict=False)
                verification_result["metrics"]["schema_compliance"] = "passed"
            except Exception as e:
                verification_result["issues"].append(f"Schema validation failed: {e}")
                verification_result["metrics"]["schema_compliance"] = "failed"
            
            # 3. ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
            self._check_missing_values(df, verification_result)
            self._check_duplicates(df, verification_result)
            self._check_timestamp_consistency(df, verification_result)
            self._check_price_validity(df, verification_result)
            
            # 4. ìƒì„¸ ê²€ì‚¬ (ì„ íƒì‚¬í•­)
            if detailed_check:
                self._check_outliers(df, verification_result)
                self._check_volume_patterns(df, verification_result)
                self._check_ohlc_relationships(df, verification_result)
            
            # 5. ì „ì²´ ìƒíƒœ íŒì •
            if not verification_result["issues"]:
                verification_result["status"] = "healthy"
            elif len(verification_result["issues"]) <= 2:
                verification_result["status"] = "warning"
                verification_result["recommendations"].append("Minor issues detected, monitoring recommended")
            else:
                verification_result["status"] = "critical"
                verification_result["recommendations"].append("Multiple issues detected, file review required")
            
            # 6. ë©”íŠ¸ë¦­ ì™„ì„±
            if 'timestamp' in df.columns:
                verification_result["metrics"]["date_range"] = {
                    "start": df['timestamp'].min().isoformat(),
                    "end": df['timestamp'].max().isoformat(),
                    "duration_days": (df['timestamp'].max() - df['timestamp'].min()).days
                }
            
            self.logger.info(f"   âœ… ê²€ì¦ ì™„ë£Œ: {verification_result['status']} ({len(verification_result['issues'])}ê°œ ì´ìŠˆ)")
            
        except Exception as e:
            verification_result["status"] = "error"
            verification_result["issues"].append(f"Verification failed: {str(e)}")
            self.logger.error(f"   âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        return verification_result
    
    def _check_missing_values(self, df: pd.DataFrame, result: Dict):
        """ê²°ì¸¡ê°’ ê²€ì‚¬"""
        missing_info = {}
        critical_columns = ['open', 'high', 'low', 'close', 'volume']
        
        for col in critical_columns:
            if col in df.columns:
                missing_count = df[col].isna().sum()
                if missing_count > 0:
                    missing_info[col] = missing_count
                    result["issues"].append(f"Missing values in {col}: {missing_count} rows")
        
        result["metrics"]["missing_values"] = missing_info
        
        # ì‹¬ê°ë„ íŒì •
        total_missing = sum(missing_info.values())
        missing_ratio = total_missing / len(df) if len(df) > 0 else 0
        
        if missing_ratio > 0.1:  # 10% ì´ìƒ ê²°ì¸¡
            result["recommendations"].append("High missing value ratio detected - data quality review needed")
    
    def _check_duplicates(self, df: pd.DataFrame, result: Dict):
        """ì¤‘ë³µ ê²€ì‚¬"""
        if 'timestamp' in df.columns and 'symbol' in df.columns:
            duplicates = df.duplicated(['timestamp', 'symbol']).sum()
        elif 'timestamp' in df.columns:
            duplicates = df.duplicated(['timestamp']).sum()
        else:
            duplicates = df.duplicated().sum()
        
        result["metrics"]["duplicate_rows"] = duplicates
        
        if duplicates > 0:
            result["issues"].append(f"Duplicate rows detected: {duplicates}")
            result["recommendations"].append("Remove duplicate entries before analysis")
    
    def _check_timestamp_consistency(self, df: pd.DataFrame, result: Dict):
        """íƒ€ì„ìŠ¤íƒ¬í”„ ì—°ì†ì„± ê²€ì‚¬"""
        if 'timestamp' not in df.columns:
            return
        
        df_sorted = df.sort_values('timestamp')
        
        # íŒŒì¼ëª…ì—ì„œ ê°„ê²© ì •ë³´ ì¶”ì¶œ ì‹œë„
        filename = result["file_name"]
        interval = None
        for candidate in ["1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h", "6h", "12h", "1d"]:
            if candidate in filename:
                interval = candidate
                break
        
        if not interval:
            result["metrics"]["timestamp_check"] = "interval_unknown"
            return
        
        # ì˜ˆìƒ ê°„ê²© (ì´ˆ ë‹¨ìœ„)
        interval_seconds = {
            "1m": 60, "3m": 180, "5m": 300, "15m": 900, "30m": 1800,
            "1h": 3600, "2h": 7200, "4h": 14400, "6h": 21600, "12h": 43200,
            "1d": 86400
        }
        
        expected_delta = interval_seconds.get(interval, 0)
        
        if expected_delta > 0 and len(df_sorted) > 1:
            # ì‹¤ì œ ì‹œê°„ ê°„ê²©ë“¤ ê³„ì‚°
            actual_deltas = df_sorted['timestamp'].diff().dt.total_seconds().dropna()
            
            # ì •ìƒ ë²”ìœ„ (Â±10% í—ˆìš©)
            tolerance = expected_delta * 0.1
            normal_deltas = actual_deltas[
                (actual_deltas >= expected_delta - tolerance) & 
                (actual_deltas <= expected_delta + tolerance)
            ]
            
            consistency_ratio = len(normal_deltas) / len(actual_deltas)
            result["metrics"]["timestamp_consistency"] = {
                "expected_interval_seconds": expected_delta,
                "consistency_ratio": consistency_ratio,
                "irregular_gaps": len(actual_deltas) - len(normal_deltas)
            }
            
            if consistency_ratio < 0.9:  # 90% ë¯¸ë§Œ ì¼ê´€ì„±
                result["issues"].append(f"Irregular timestamp intervals detected (consistency: {consistency_ratio:.1%})")
                result["recommendations"].append("Check for missing candles or data collection issues")
    
    def _check_price_validity(self, df: pd.DataFrame, result: Dict):
        """ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        price_issues = []
        price_columns = ['open', 'high', 'low', 'close']
        
        for col in price_columns:
            if col not in df.columns:
                continue
                
            # 0 ì´í•˜ ê°’ ì²´í¬
            zero_or_negative = (df[col] <= 0).sum()
            if zero_or_negative > 0:
                price_issues.append(f"{col}: {zero_or_negative} zero/negative values")
        
        # OHLC ê´€ê³„ ê²€ì¦ (ìƒ˜í”Œë§)
        if all(col in df.columns for col in price_columns):
            sample_size = min(1000, len(df))
            sample_df = df.sample(n=sample_size) if len(df) > sample_size else df
            
            invalid_ohlc = sample_df[
                ~((sample_df['low'] <= sample_df['open']) & (sample_df['open'] <= sample_df['high']) &
                  (sample_df['low'] <= sample_df['close']) & (sample_df['close'] <= sample_df['high']))
            ]
            
            if not invalid_ohlc.empty:
                price_issues.append(f"Invalid OHLC relationships: {len(invalid_ohlc)} rows")
        
        result["metrics"]["price_issues"] = price_issues
        
        for issue in price_issues:
            result["issues"].append(f"Price validation: {issue}")
    
    def _check_outliers(self, df: pd.DataFrame, result: Dict):
        """ì´ìƒê°’ ê²€ì‚¬ (ìƒì„¸ ê²€ì‚¬)"""
        outlier_info = {}
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
        
        for col in numeric_columns:
            if col not in df.columns:
                continue
                
            values = df[col].dropna()
            if len(values) < 10:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
                continue
            
            # IQR ê¸°ë°˜ ì´ìƒê°’ íƒì§€
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            
            outliers = values[(values < lower_bound) | (values > upper_bound)]
            
            if not outliers.empty:
                outlier_info[col] = {
                    "count": len(outliers),
                    "ratio": len(outliers) / len(values),
                    "extreme_values": {
                        "min": float(outliers.min()),
                        "max": float(outliers.max())
                    }
                }
        
        result["metrics"]["outliers"] = outlier_info
        
        # ì‹¬ê°í•œ ì´ìƒê°’ì´ ë§ìœ¼ë©´ ê²½ê³ 
        for col, info in outlier_info.items():
            if info["ratio"] > 0.05:  # 5% ì´ìƒ ì´ìƒê°’
                result["issues"].append(f"High outlier ratio in {col}: {info['ratio']:.1%}")
    
    def _check_volume_patterns(self, df: pd.DataFrame, result: Dict):
        """ê±°ë˜ëŸ‰ íŒ¨í„´ ê²€ì‚¬"""
        if 'volume' not in df.columns:
            return
        
        volume = df['volume'].dropna()
        if len(volume) < 10:
            return
        
        # ê±°ë˜ëŸ‰ 0ì¸ ìº”ë“¤ ë¹„ìœ¨
        zero_volume_ratio = (volume == 0).sum() / len(volume)
        
        # í‰ê·  ëŒ€ë¹„ ê·¹ë‹¨ì  ê±°ë˜ëŸ‰ ë¹„ìœ¨
        mean_volume = volume.mean()
        extreme_volume_ratio = (volume > mean_volume * 10).sum() / len(volume)
        
        volume_metrics = {
            "zero_volume_ratio": zero_volume_ratio,
            "extreme_volume_ratio": extreme_volume_ratio,
            "average_volume": float(mean_volume)
        }
        
        result["metrics"]["volume_patterns"] = volume_metrics
        
        if zero_volume_ratio > 0.1:  # 10% ì´ìƒ ê±°ë˜ëŸ‰ 0
            result["issues"].append(f"High zero-volume ratio: {zero_volume_ratio:.1%}")
        
        if extreme_volume_ratio > 0.02:  # 2% ì´ìƒ ê·¹ë‹¨ì  ê±°ë˜ëŸ‰
            result["issues"].append(f"Frequent volume spikes detected: {extreme_volume_ratio:.1%}")
    
    def _check_ohlc_relationships(self, df: pd.DataFrame, result: Dict):
        """OHLC ê´€ê³„ ìƒì„¸ ê²€ì‚¬"""
        price_columns = ['open', 'high', 'low', 'close']
        if not all(col in df.columns for col in price_columns):
            return
        
        # ë‹¤ì–‘í•œ OHLC ê´€ê³„ ê²€ì¦
        relationship_checks = {
            "high_is_max": ((df['high'] >= df['open']) & 
                          (df['high'] >= df['low']) & 
                          (df['high'] >= df['close'])).all(),
            "low_is_min": ((df['low'] <= df['open']) & 
                         (df['low'] <= df['high']) & 
                         (df['low'] <= df['close'])).all(),
            "reasonable_ranges": (df['high'] - df['low']).quantile(0.95) < df['close'].mean() * 0.2  # 95% ìº”ë“¤ì´ í‰ê· ê°€ 20% ë²”ìœ„ ë‚´
        }
        
        result["metrics"]["ohlc_relationships"] = relationship_checks
        
        for check_name, passed in relationship_checks.items():
            if not passed:
                result["issues"].append(f"OHLC relationship check failed: {check_name}")
    
    def verify_multiple_files(
        self,
        file_pattern: str = "*.parquet",
        detailed_check: bool = True,
    ) -> Dict[str, Any]:
        """
        ğŸ“¦ ì—¬ëŸ¬ íŒŒì¼ ì¼ê´„ ê²€ì¦
        
        Args:
            file_pattern: ê²€ì¦í•  íŒŒì¼ íŒ¨í„´ (ì˜ˆ: "*.parquet", "*btc*.csv")
            detailed_check: ìƒì„¸ ê²€ì‚¬ ì—¬ë¶€
            
        Returns:
            ì „ì²´ ê²€ì¦ ê²°ê³¼ ìš”ì•½
        """
        self.logger.info(f"ğŸš€ ì¼ê´„ ê²€ì¦ ì‹œì‘: {file_pattern}")
        
        files_to_verify = list(self.processed_dir.glob(file_pattern))
        
        if not files_to_verify:
            self.logger.warning(f"ê²€ì¦í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_pattern}")
            return {"status": "no_files", "files_checked": 0}
        
        verification_results = []
        summary = {
            "verification_timestamp": datetime.now().isoformat(),
            "total_files": len(files_to_verify),
            "files_checked": 0,
            "status_counts": {"healthy": 0, "warning": 0, "critical": 0, "error": 0},
            "common_issues": {}
        }
        
        for file_path in files_to_verify:
            result = self.verify_single_file(file_path, detailed_check)
            verification_results.append(result)
            
            summary["files_checked"] += 1
            status = result.get("status", "error")
            summary["status_counts"][status] = summary["status_counts"].get(status, 0) + 1
            
            # ê³µí†µ ì´ìŠˆ ì§‘ê³„
            for issue in result.get("issues", []):
                issue_type = issue.split(":")[0] if ":" in issue else issue
                summary["common_issues"][issue_type] = summary["common_issues"].get(issue_type, 0) + 1
        
        # ìš”ì•½ ì¶œë ¥
        self.logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: {summary['files_checked']}ê°œ íŒŒì¼")
        for status, count in summary["status_counts"].items():
            if count > 0:
                self.logger.info(f"   â€¢ {status}: {count}ê°œ")
        
        return summary
    
    def get_health_dashboard(self) -> Dict[str, Any]:
        """ğŸ“ˆ ë°ì´í„° ìƒíƒœ ëŒ€ì‹œë³´ë“œ"""
        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "file_counts": {
                "processed": len(list(self.processed_dir.glob("*")))
            },
            "recent_verifications": []
        }
        
        return dashboard


# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_verify_all(detailed: bool = False) -> Dict:
    """ëª¨ë“  processed íŒŒì¼ ë¹ ë¥¸ ê²€ì¦"""
    verifier = DataIntegrityVerifier()
    return verifier.verify_multiple_files("*", detailed_check=detailed)


def health_check() -> None:
    """ë°ì´í„° ìƒíƒœ ê°„ë‹¨ ì²´í¬"""
    verifier = DataIntegrityVerifier()
    dashboard = verifier.get_health_dashboard()
    
    print("ğŸ“Š ë°ì´í„° ìƒíƒœ ì²´í¬")
    print(f"  â€¢ Processed íŒŒì¼: {dashboard['file_counts']['processed']}ê°œ")
    
    if dashboard["recent_verifications"]:
        latest = dashboard["recent_verifications"][0]
        print(f"  â€¢ ìµœê·¼ ê²€ì¦: {latest['files_checked']}ê°œ íŒŒì¼")
        for status, count in latest["status_counts"].items():
            if count > 0:
                print(f"    - {status}: {count}ê°œ")


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    print("ğŸ” DataIntegrityVerifier í…ŒìŠ¤íŠ¸")
    
    verifier = DataIntegrityVerifier()
    
    # ì „ì²´ ìƒíƒœ ì²´í¬
    health_check()
    
    # ìƒ˜í”Œ ê²€ì¦ (íŒŒì¼ì´ ìˆë‹¤ë©´)
    processed_files = list(verifier.processed_dir.glob("*.parquet"))
    if processed_files:
        print(f"\nğŸ§ª ìƒ˜í”Œ íŒŒì¼ ê²€ì¦: {processed_files[0].name}")
        result = verifier.verify_single_file(processed_files[0])
        print(f"ê²°ê³¼: {result['status']}")
        if result["issues"]:
            print("ì´ìŠˆ:")
            for issue in result["issues"][:3]:  # ìµœëŒ€ 3ê°œë§Œ ì¶œë ¥
                print(f"  - {issue}")
    else:
        print("\nâš ï¸ ê²€ì¦í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. build_datasets.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ë³´ì„¸ìš”.")