"""
ğŸ“¦ ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ êµ¬ì¶•ê¸°

fetcherë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë“¤ì„ íŒŒì¼ë¡œ ì •ë¦¬í•˜ê³  ì €ì¥í•˜ëŠ” ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
1. ì—¬ëŸ¬ ì‹¬ë³¼ / ì—¬ëŸ¬ íƒ€ì„í”„ë ˆì„ í•œêº¼ë²ˆì— ìƒì„±
2. processed í´ë”ì— íŒŒì¼ ì €ì¥  
3. ì €ì¥ í¬ë§· ë³€í™˜ (CSV, Parquet, Pickle)
4. ë©”íƒ€ë°ì´í„° ë¡œê·¸ ì‘ì„±

íë¦„: fetch_market_data.py â†’ build_datasets.py â†’ verify_integrity.py
"""

import json
import pandas as pd
import pytz
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Union, Tuple
import os
import time

from .fetch_market_data import UpbitDataFetcher, MarketDataFetcher
from .schema import candles_to_dataframe, ensure_candle_schema


class DatasetBuilder:
    """
    ğŸ—ï¸ ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ êµ¬ì¶•ê¸°
    
    ì—¬ëŸ¬ ì‹¬ë³¼ê³¼ íƒ€ì„í”„ë ˆì„ì„ ì¡°í•©í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, base_data_dir: str = "backtest_data"):
        self.base_dir = Path(base_data_dir)
        self.processed_dir = self.base_dir / "processed" 
        self.metadata_dir = self.base_dir / "metadata"
        
        self._ensure_directories()
        self.fetcher = MarketDataFetcher()
        
    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í„°ë¦¬ë“¤ ìƒì„±"""
        for dir_path in [self.processed_dir, self.metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def _normalize_symbol_for_filename(self, symbol: str) -> str:
        """ì‹¬ë³¼ëª…ì„ íŒŒì¼ëª…ì— ì í•©í•˜ê²Œ ë³€í™˜"""
        return symbol.replace('-', '_').replace('/', '_').lower()
    
    def _generate_filename(
        self, 
        symbol: str, 
        interval: str, 
        start_date: datetime,
        end_date: datetime,
        file_type: str = "processed"
    ) -> str:
        """íŒŒì¼ëª… ìƒì„± (ê°„ê²© ì •ë³´ í¬í•¨)"""
        norm_symbol = self._normalize_symbol_for_filename(symbol)
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        
        return f"{norm_symbol}_{interval}_{start_str}_{end_str}_{file_type}"
    
    def build_single_dataset(
        self,
        exchange: str,
        symbol: str,
        interval: str,
        start_date: datetime,
        end_date: datetime,
        save_formats: List[str] = ["parquet", "csv"]
    ) -> Dict:
        """
        ğŸ¯ ë‹¨ì¼ ì‹¬ë³¼/ê°„ê²© ë°ì´í„°ì…‹ êµ¬ì¶•
        
        Args:
            exchange: ê±°ë˜ì†Œëª… ("upbit" ë“±)
            symbol: ë§ˆì¼“ ì½”ë“œ ("KRW-BTC" ë“±)
            interval: ìº”ë“¤ ê°„ê²© ("1m", "5m", "1h" ë“±)
            start_date: ìˆ˜ì§‘ ì‹œì‘ì¼
            end_date: ìˆ˜ì§‘ ì¢…ë£Œì¼
            save_formats: ì €ì¥í•  í¬ë§· ë¦¬ìŠ¤íŠ¸
            
        Returns:
            êµ¬ì¶• ê²°ê³¼ ì •ë³´
        """
        print(f"Dataset build starting: {symbol} {interval} ({start_date.date()} ~ {end_date.date()})")
        
        build_info = {
            "symbol": symbol,
            "exchange": exchange, 
            "interval": interval,
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "build_timestamp": datetime.now().isoformat(),
            "status": "started",
            "candles_collected": 0,
            "files_created": [],
            "errors": []
        }
        
        try:
            # 1. ë‚ ì§œ íƒ€ì„ì¡´ ì²˜ë¦¬ 
            if start_date and start_date.tzinfo is None:
                start_date = pytz.UTC.localize(start_date)
            if end_date and end_date.tzinfo is None:
                end_date = pytz.UTC.localize(end_date)
                
            # 2. ë°ì´í„° ìˆ˜ì§‘
            fetcher = self.fetcher.get_fetcher(exchange)
            
            if hasattr(fetcher, 'fetch_candles_bulk'):
                candles = fetcher.fetch_candles_bulk(symbol, interval, start_date, end_date)
            else:
                # bulk ì§€ì› ì•ˆ í•˜ë©´ ì¼ë°˜ fetchë¡œ ëŒ€ì²´
                candles = fetcher.fetch_candles(symbol, interval, count=200)
            
            build_info["candles_collected"] = len(candles)
            
            if not candles:
                build_info["status"] = "no_data"
                build_info["errors"].append("No candles received from API")
                return build_info
            
            # 2. DataFrame ë³€í™˜ ë° ê²€ì¦
            df = candles_to_dataframe(candles)
            df = ensure_candle_schema(df)
            
            print(f"   OK {len(df)} candles collected")
            
            # 3. ê°€ê³µ ë°ì´í„° ì €ì¥ (ì—¬ëŸ¬ í¬ë§·)
            processed_filename = self._generate_filename(symbol, interval, start_date, end_date, "processed")
            
            for fmt in save_formats:
                if fmt == "parquet":
                    file_path = self.processed_dir / f"{processed_filename}.parquet"
                    df.to_parquet(file_path, index=False)
                elif fmt == "csv":
                    file_path = self.processed_dir / f"{processed_filename}.csv"
                    df.to_csv(file_path, index=False)
                elif fmt == "pickle":
                    file_path = self.processed_dir / f"{processed_filename}.pkl"
                    df.to_pickle(file_path)
                else:
                    build_info["errors"].append(f"Unsupported format: {fmt}")
                    continue
                
                build_info["files_created"].append(str(file_path))
                print(f"   Processed data saved: {file_path.name}")
            
            # 5. ë©”íƒ€ë°ì´í„° ë¡œê·¸ ì‘ì„±
            metadata = {
                **build_info,
                "data_quality": {
                    "date_range_actual": {
                        "start": df['timestamp'].min().isoformat(),
                        "end": df['timestamp'].max().isoformat()
                    },
                    "missing_candles": self._detect_missing_candles(df, interval),
                    "price_stats": {
                        "min_price": float(df['close'].min()),
                        "max_price": float(df['close'].max()),
                        "avg_price": float(df['close'].mean())
                    },
                    "volume_stats": {
                        "total_volume": float(df['volume'].sum()),
                        "avg_volume": float(df['volume'].mean())
                    }
                }
            }
            
            metadata_path = self.metadata_dir / f"{processed_filename}_meta.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            build_info["files_created"].append(str(metadata_path))
            build_info["status"] = "completed"
            
            print(f"   BUILD COMPLETED! {len(build_info['files_created'])} files created")
            
        except Exception as e:
            build_info["status"] = "error"
            build_info["errors"].append(str(e))
            print(f"   ERROR occurred: {e}")
        
        return build_info
    
    def build_multiple_datasets(
        self,
        pairs_config: List[Dict],
        batch_delay: float = 1.0
    ) -> List[Dict]:
        """
        ğŸ“¦ ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì¼ê´„ êµ¬ì¶•
        
        Args:
            pairs_config: êµ¬ì¶•í•  ë°ì´í„°ì…‹ ì„¤ì • ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: [{"exchange": "upbit", "symbol": "KRW-BTC", "interval": "1h", ...}, ...]
            batch_delay: ê° êµ¬ì¶• ì‚¬ì´ ë”œë ˆì´ (ì´ˆ)
            
        Returns:
            ê° êµ¬ì¶• ì‘ì—…ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸš€ ì¼ê´„ ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œì‘ - ì´ {len(pairs_config)}ê°œ ì‘ì—…")
        
        results = []
        total_start_time = time.time()
        
        for i, config in enumerate(pairs_config, 1):
            print(f"\nğŸ“‹ ì‘ì—… {i}/{len(pairs_config)}")
            
            result = self.build_single_dataset(**config)
            results.append(result)
            
            # ë‹¤ìŒ ì‘ì—… ì „ ë”œë ˆì´ (API ë¶€í•˜ ë°©ì§€)
            if i < len(pairs_config):
                print(f"   â³ {batch_delay}ì´ˆ ëŒ€ê¸°...")
                time.sleep(batch_delay)
        
        # ì „ì²´ ìš”ì•½
        total_time = time.time() - total_start_time
        successful = len([r for r in results if r["status"] == "completed"])
        failed = len(results) - successful
        
        print(f"\nğŸ“Š ì¼ê´„ êµ¬ì¶• ì™„ë£Œ!")
        print(f"   â€¢ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   â€¢ ì„±ê³µ: {successful}ê°œ, ì‹¤íŒ¨: {failed}ê°œ")
        
        if failed > 0:
            print("   âŒ ì‹¤íŒ¨í•œ ì‘ì—…ë“¤:")
            for result in results:
                if result["status"] != "completed":
                    print(f"     - {result['symbol']} {result['interval']}: {result.get('errors', [])}")
        
        # ì „ì²´ ìš”ì•½ ë©”íƒ€ë°ì´í„° ì €ì¥
        summary_path = self.metadata_dir / f"build_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        summary = {
            "batch_info": {
                "total_jobs": len(pairs_config),
                "successful": successful,
                "failed": failed,
                "total_time_seconds": total_time,
                "timestamp": datetime.now().isoformat()
            },
            "job_results": results
        }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"   ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {summary_path.name}")
        
        return results
    
    def _detect_missing_candles(self, df: pd.DataFrame, interval: str) -> int:
        """ìº”ë“¤ ëˆ„ë½ ê°œìˆ˜ ì¶”ì •"""
        if len(df) < 2:
            return 0
        
        # ê°„ê²©ë³„ ì˜ˆìƒ ì‹œê°„ ì°¨ì´ (ë¶„ ë‹¨ìœ„)
        interval_minutes = {
            "1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30,
            "1h": 60, "2h": 120, "4h": 240, "6h": 360, "12h": 720,
            "1d": 1440, "1w": 10080
        }
        
        if interval not in interval_minutes:
            return 0  # ì•Œ ìˆ˜ ì—†ëŠ” ê°„ê²©
        
        expected_minutes = interval_minutes[interval]
        df_sorted = df.sort_values('timestamp')
        
        start_time = df_sorted['timestamp'].iloc[0]
        end_time = df_sorted['timestamp'].iloc[-1]
        
        # ì˜ˆìƒ ìº”ë“¤ ìˆ˜
        total_minutes = (end_time - start_time).total_seconds() / 60
        expected_candles = int(total_minutes / expected_minutes) + 1
        
        # ì‹¤ì œ ìº”ë“¤ ìˆ˜ì™€ ë¹„êµ
        actual_candles = len(df)
        missing = max(0, expected_candles - actual_candles)
        
        return missing
    
    def get_build_status(self) -> Dict:
        """êµ¬ì¶•ëœ ë°ì´í„°ì…‹ í˜„í™© ì¡°íšŒ"""
        status = {
            "directories": {
                "processed_parquet": len(list(self.processed_dir.glob("*.parquet"))), 
                "processed_csv": len(list(self.processed_dir.glob("*.csv"))),
                "metadata": len(list(self.metadata_dir.glob("*.json")))
            },
            "recent_builds": []
        }
        
        # ìµœê·¼ êµ¬ì¶• ì‘ì—…ë“¤ (ë©”íƒ€ë°ì´í„° ê¸°ì¤€)
        metadata_files = sorted(
            self.metadata_dir.glob("*_meta.json"), 
            key=lambda x: x.stat().st_mtime, 
            reverse=True
        )[:10]
        
        for meta_file in metadata_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
                    status["recent_builds"].append({
                        "symbol": meta.get("symbol"),
                        "interval": meta.get("interval"),
                        "candles": meta.get("candles_collected", 0),
                        "status": meta.get("status"),
                        "timestamp": meta.get("build_timestamp")
                    })
            except Exception:
                continue
        
        return status


# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_build_upbit_dataset(
    symbols: List[str],
    intervals: List[str],
    days_back: int = 30,
    **kwargs
) -> List[Dict]:
    """
    âš¡ Upbit ë°ì´í„°ì…‹ ë¹ ë¥¸ êµ¬ì¶•
    
    Args:
        symbols: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["KRW-BTC", "KRW-ETH"])
        intervals: ê°„ê²© ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["1h", "1d"])
        days_back: ë©°ì¹  ì „ë¶€í„° ìˆ˜ì§‘í• ì§€
    """
    builder = DatasetBuilder()
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_back)
    
    configs = []
    for symbol in symbols:
        for interval in intervals:
            configs.append({
                "exchange": "upbit",
                "symbol": symbol,
                "interval": interval,
                "start_date": start_date,
                "end_date": end_date,
                **kwargs
            })
    
    return builder.build_multiple_datasets(configs)


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    print("ğŸ”§ DatasetBuilder í…ŒìŠ¤íŠ¸")
    
    # ê°„ë‹¨í•œ êµ¬ì¶• í…ŒìŠ¤íŠ¸
    builder = DatasetBuilder()
    
    # ë‹¨ì¼ ë°ì´í„°ì…‹ êµ¬ì¶• (ìµœê·¼ 7ì¼)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=7)
    
    result = builder.build_single_dataset(
        exchange="upbit",
        symbol="KRW-BTC", 
        interval="1h",
        start_date=start_date,
        end_date=end_date,
        save_formats=["parquet"]
    )
    
    print(f"\nêµ¬ì¶• ê²°ê³¼: {result['status']}")
    if result["files_created"]:
        print("ìƒì„±ëœ íŒŒì¼ë“¤:")
        for file in result["files_created"]:
            print(f"  - {file}")
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    status = builder.get_build_status()
    print(f"\nğŸ“Š í˜„ì¬ ë°ì´í„°ì…‹ ìƒíƒœ:")
    for dir_name, count in status["directories"].items():
        print(f"  {dir_name}: {count}ê°œ íŒŒì¼")