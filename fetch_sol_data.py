"""
SOL ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
2ë…„ì¹˜ 4ì‹œê°„ ë‹¨ìœ„ OHLCV ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°±í…ŒìŠ¤íŠ¸ìš© CSVë¡œ ì €ì¥
"""

import pandas as pd
import numpy as np
import requests
import time
from datetime import datetime, timedelta
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

class SOLDataFetcher:
    def __init__(self):
        self.base_url = "https://api.upbit.com/v1"
        self.symbol = "KRW-SOL"
        self.interval = "240"  # 4ì‹œê°„
        self.output_file = "data/ohlcv/KRW-SOL_240m.csv"
        
    def fetch_candles(self, count=200, to_time=None):
        """ì—…ë¹„íŠ¸ APIì—ì„œ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘"""
        url = f"{self.base_url}/candles/minutes/{self.interval}"
        
        params = {
            "market": self.symbol,
            "count": count
        }
        
        if to_time:
            params["to"] = to_time
            
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
            
    def collect_historical_data(self, start_date="2023-11-07", days=730):  # BTCì™€ ë™ì¼í•œ ì‹œì‘ì¼
        """2ë…„ì¹˜ SOL ë°ì´í„° ìˆ˜ì§‘ (BTC ì‹œì‘ì¼ë¶€í„°)"""
        logger.info(f"SOL ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_date}ë¶€í„° {days}ì¼ì¹˜")
        
        all_data = []
        
        # ì‹œì‘ì¼ ì„¤ì •
        from datetime import datetime
        start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
        current_time = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        
        # ìµœëŒ€ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ìº”ë“¤ ìˆ˜ (ì—…ë¹„íŠ¸ ì œí•œ)
        max_candles = 200
        collected_candles = 0
        
        while collected_candles < days * 6:  # 4ì‹œê°„ ê°„ê²©ì´ë¯€ë¡œ í•˜ë£¨ 6ê°œ
            # í•œ ë²ˆì— ìµœëŒ€ 200ê°œì”© ìˆ˜ì§‘
            batch_size = min(max_candles, (days * 6) - collected_candles)
            
            logger.info(f"ë°°ì¹˜ ìˆ˜ì§‘: {collected_candles}/{days * 6} ({collected_candles/(days * 6)*100:.1f}%)")
            
            candles = self.fetch_candles(count=batch_size, to_time=current_time)
            
            if not candles:
                logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                break
                
            # ì‹œì‘ì¼ ì´ì „ ë°ì´í„°ëŠ” ì œì™¸
            valid_candles = []
            for candle in candles:
                candle_time = datetime.strptime(candle['candle_date_time_kst'], "%Y-%m-%dT%H:%M:%S")
                if candle_time >= start_datetime:
                    valid_candles.append(candle)
            
            # ë°ì´í„° ì²˜ë¦¬
            for candle in valid_candles:
                all_data.append({
                    'timestamp': candle['candle_date_time_kst'],
                    'open': float(candle['opening_price']),
                    'high': float(candle['high_price']),
                    'low': float(candle['low_price']),
                    'close': float(candle['trade_price']),
                    'volume': float(candle['candle_acc_trade_volume'])
                })
            
            collected_candles += len(candles)
            
            # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•œ ì‹œê°„ ì„¤ì • (ê°€ì¥ ì˜¤ë˜ëœ ìº”ë“¤ì˜ ì‹œê°„)
            if candles:
                current_time = candles[-1]['candle_date_time_kst']
                
                # ì‹œì‘ì¼ë³´ë‹¤ ì´ì „ê¹Œì§€ ê°”ìœ¼ë©´ ì¤‘ë‹¨
                oldest_time = datetime.strptime(current_time, "%Y-%m-%dT%H:%M:%S")
                if oldest_time < start_datetime:
                    logger.info(f"ëª©í‘œ ì‹œì‘ì¼({start_date}) ë„ë‹¬. ìˆ˜ì§‘ ì™„ë£Œ")
                    break
            
            # API ì œí•œ ì¤€ìˆ˜ (ì´ˆë‹¹ 10íšŒ)
            time.sleep(0.1)
            
        logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_data)}ê°œ ìº”ë“¤")
        return all_data
    
    def save_to_csv(self, data):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        df = pd.DataFrame(data)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ë° ì •ë ¬
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(self.output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # CSV ì €ì¥
        df.to_csv(output_path, index=False)
        
        logger.info(f"SOL ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
        logger.info(f"ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        logger.info(f"ì´ {len(df)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        self.validate_data(df)
        
        return df
    
    def validate_data(self, df):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        logger.info("=== ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ===")
        
        # ê¸°ë³¸ í†µê³„
        logger.info(f"ê°€ê²© ë²”ìœ„: {df['close'].min():,.0f} ~ {df['close'].max():,.0f} KRW")
        logger.info(f"í‰ê·  ê±°ë˜ëŸ‰: {df['volume'].mean():.2f}")
        logger.info(f"ëˆ„ë½ ë°ì´í„°: {df.isnull().sum().sum()}ê°œ")
        
        # ì‹œê°„ ê°„ê²© ê²€ì¦ (4ì‹œê°„ = 240ë¶„)
        time_diffs = df['timestamp'].diff().dt.total_seconds() / 60
        expected_interval = 240
        
        correct_intervals = (time_diffs == expected_interval).sum()
        total_intervals = len(time_diffs) - 1
        
        logger.info(f"ì‹œê°„ ê°„ê²© ì •í™•ë„: {correct_intervals}/{total_intervals} ({correct_intervals/total_intervals*100:.1f}%)")
        
        # ê°€ê²© ì´ìƒì¹˜ ê²€ì¦
        price_changes = df['close'].pct_change().abs()
        extreme_changes = (price_changes > 0.3).sum()  # 30% ì´ìƒ ë³€ë™
        
        logger.info(f"ê·¹ë‹¨ì  ê°€ê²© ë³€ë™ (>30%): {extreme_changes}ê°œ")
        
        if extreme_changes > 0:
            logger.warning("ê·¹ë‹¨ì  ê°€ê²© ë³€ë™ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ í•„ìš”")
            
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        logger.info("ğŸ”— SOL ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        # ë°ì´í„° ìˆ˜ì§‘
        data = self.collect_historical_data()
        
        if not data:
            logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return None
            
        # CSV ì €ì¥
        df = self.save_to_csv(data)
        
        logger.info("âœ… SOL ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        return df

if __name__ == "__main__":
    fetcher = SOLDataFetcher()
    sol_data = fetcher.run()
    
    if sol_data is not None:
        print(f"\nğŸ“Š SOL ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        print(sol_data.head(10))
        print(f"\nğŸ“ˆ ìµœì‹  SOL ê°€ê²©: {sol_data['close'].iloc[-1]:,.0f} KRW")