"""
ğŸ“‚ ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë”

processed/ í´ë”ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ í›„ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì— ì „ë‹¬í•©ë‹ˆë‹¤.
ëª¨ë“  ë°ì´í„°ëŠ” í‘œì¤€ ìŠ¤í‚¤ë§ˆë¥¼ ê±°ì³ì•¼ ë°±í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Union, Tuple
import glob
import pickle

from data_tools.schema import (
    Candle, ensure_candle_schema, validate_candle_data, 
    candles_to_dataframe, dataframe_to_candles, REQUIRED_CANDLE_COLUMNS
)


class BacktestDataLoader:
    """
    ğŸ”„ ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë”
    
    ì—­í• :
    1. processed/ í´ë”ì—ì„œ ë°ì´í„° íŒŒì¼ ì½ê¸°
    2. í‘œì¤€ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ê°•ì œ ì ìš©  
    3. ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì´ ì›í•˜ëŠ” í˜•íƒœë¡œ ë°ì´í„° ì œê³µ
    4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ë¡œë”© ì§€ì›
    """
    
    def __init__(self, data_dir: str = "backtest_data/processed"):
        self.data_dir = Path(data_dir)
        self.cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ
        self._ensure_data_dir()
    
    def _ensure_data_dir(self):
        """ë°ì´í„° ë””ë ‰í„°ë¦¬ ìƒì„±"""
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def list_available_data(self) -> Dict[str, List[str]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ëª©ë¡"""
        files = {}
        
        # CSV íŒŒì¼ë“¤
        csv_files = list(self.data_dir.glob("*.csv"))
        if csv_files:
            files['csv'] = [f.name for f in csv_files]
        
        # Parquet íŒŒì¼ë“¤  
        parquet_files = list(self.data_dir.glob("*.parquet"))
        if parquet_files:
            files['parquet'] = [f.name for f in parquet_files]
        
        # Pickle íŒŒì¼ë“¤
        pickle_files = list(self.data_dir.glob("*.pkl"))
        if pickle_files:
            files['pickle'] = [f.name for f in pickle_files]
        
        return files
    
    def load_candles_from_file(
        self, 
        filename: str,
        symbol_filter: Optional[str] = None,
        date_range: Optional[Tuple[datetime, datetime]] = None,
        validate: bool = True
    ) -> pd.DataFrame:
        """
        ğŸ“Š íŒŒì¼ì—ì„œ ìº”ë“¤ ë°ì´í„° ë¡œë“œ ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦
        
        Args:
            filename: ë°ì´í„° íŒŒì¼ëª…
            symbol_filter: íŠ¹ì • ì‹¬ë³¼ë§Œ í•„í„°ë§
            date_range: (ì‹œì‘ì¼, ì¢…ë£Œì¼) ë²”ìœ„ í•„í„°ë§  
            validate: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì—¬ë¶€
            
        Returns:
            ê²€ì¦ëœ ìº”ë“¤ DataFrame
        """
        file_path = self.data_dir / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"Data file not found: {file_path}")
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{filename}_{symbol_filter}_{date_range}"
        if cache_key in self.cache:
            return self.cache[cache_key].copy()
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë”©
        if filename.endswith('.csv'):
            df = pd.read_csv(file_path, parse_dates=['timestamp'])
        elif filename.endswith('.parquet'):
            df = pd.read_parquet(file_path)
        elif filename.endswith('.pkl'):
            df = pd.read_pickle(file_path)
        else:
            raise ValueError(f"Unsupported file format: {filename}")
        
        if df.empty:
            return df
        
        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ê°•ì œ
        if validate:
            try:
                df = ensure_candle_schema(df, strict=True)
            except Exception as e:
                raise ValueError(f"Schema validation failed for {filename}: {e}")
        
        # í•„í„°ë§ ì ìš©
        if symbol_filter:
            df = df[df['symbol'] == symbol_filter]
        
        if date_range:
            start_date, end_date = date_range
            df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]
        
        # ì‹œê°„ìˆœ ì •ë ¬
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤)
        if len(self.cache) < 10:  # ìµœëŒ€ 10ê°œ íŒŒì¼ ìºì‹œ
            self.cache[cache_key] = df.copy()
        
        return df
    
    def load_multiple_symbols(
        self,
        symbols: List[str],
        start_date: datetime,
        end_date: datetime,
        interval: str = "1h"
    ) -> Dict[str, pd.DataFrame]:
        """
        ğŸ“ˆ ì—¬ëŸ¬ ì‹¬ë³¼ì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ë¡œë“œ
        
        Returns:
            {symbol: DataFrame} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        result = {}
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤ í™•ì¸
        available_files = self.list_available_data()
        all_files = []
        for file_type, files in available_files.items():
            all_files.extend(files)
        
        for symbol in symbols:
            symbol_data = None
            
            # ì‹¬ë³¼ë³„ ì „ìš© íŒŒì¼ ì°¾ê¸°
            symbol_files = [f for f in all_files if symbol.replace('-', '_') in f]
            
            if symbol_files:
                # ê°€ì¥ ì ì ˆí•œ íŒŒì¼ ì„ íƒ (ê°„ê²© ë§¤ì¹­)
                best_file = None
                for file in symbol_files:
                    if interval in file:
                        best_file = file
                        break
                
                if not best_file:
                    best_file = symbol_files[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
                
                try:
                    symbol_data = self.load_candles_from_file(
                        best_file,
                        symbol_filter=symbol,
                        date_range=(start_date, end_date)
                    )
                except Exception as e:
                    print(f"Warning: Failed to load {symbol} from {best_file}: {e}")
            
            if symbol_data is None or symbol_data.empty:
                # í†µí•© íŒŒì¼ì—ì„œ ì°¾ê¸°
                for file in all_files:
                    try:
                        symbol_data = self.load_candles_from_file(
                            file,
                            symbol_filter=symbol,
                            date_range=(start_date, end_date)
                        )
                        if not symbol_data.empty:
                            break
                    except Exception:
                        continue
            
            result[symbol] = symbol_data if symbol_data is not None else pd.DataFrame()
        
        return result
    
    def create_batch_iterator(
        self,
        filename: str,
        batch_size_days: int = 30,
        overlap_days: int = 1,
        **kwargs
    ):
        """
        ğŸ”„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì´í„°ë ˆì´í„°
        
        í° ë°ì´í„°ì…‹ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
        """
        # ì „ì²´ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        df_sample = self.load_candles_from_file(filename, **kwargs)
        if df_sample.empty:
            return
        
        start_date = df_sample['timestamp'].min()
        end_date = df_sample['timestamp'].max()
        
        current_start = start_date
        batch_delta = timedelta(days=batch_size_days)
        overlap_delta = timedelta(days=overlap_days)
        
        while current_start < end_date:
            current_end = min(current_start + batch_delta, end_date)
            
            # ë°°ì¹˜ ë¡œë“œ
            batch_df = self.load_candles_from_file(
                filename,
                date_range=(current_start, current_end),
                **kwargs
            )
            
            if not batch_df.empty:
                yield batch_df
            
            # ë‹¤ìŒ ë°°ì¹˜ ì‹œì‘ì  (ì˜¤ë²„ë© ê³ ë ¤)
            current_start = current_end - overlap_delta
    
    def get_data_quality_report(self, filename: str) -> Dict:
        """
        ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
        """
        try:
            df = self.load_candles_from_file(filename, validate=False)
            
            if df.empty:
                return {"status": "empty", "message": "No data in file"}
            
            # ê¸°ë³¸ í†µê³„
            report = {
                "file_info": {
                    "filename": filename,
                    "total_rows": len(df),
                    "memory_usage_mb": df.memory_usage(deep=True).sum() / 1024 / 1024
                },
                "date_range": {
                    "start": df['timestamp'].min() if 'timestamp' in df.columns else None,
                    "end": df['timestamp'].max() if 'timestamp' in df.columns else None,
                    "days": None
                },
                "symbols": list(df['symbol'].unique()) if 'symbol' in df.columns else [],
                "schema_compliance": {
                    "missing_columns": [],
                    "extra_columns": [],
                    "type_issues": []
                },
                "data_quality": {
                    "missing_values": {},
                    "duplicate_rows": 0,
                    "price_anomalies": []
                }
            }
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            if report["date_range"]["start"] and report["date_range"]["end"]:
                report["date_range"]["days"] = (
                    report["date_range"]["end"] - report["date_range"]["start"]
                ).days
            
            # ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ ì²´í¬
            required_cols = set(REQUIRED_CANDLE_COLUMNS)
            actual_cols = set(df.columns)
            
            report["schema_compliance"]["missing_columns"] = list(required_cols - actual_cols)
            report["schema_compliance"]["extra_columns"] = list(actual_cols - required_cols)
            
            # ê²°ì¸¡ê°’ ì²´í¬
            for col in df.columns:
                missing_count = df[col].isna().sum()
                if missing_count > 0:
                    report["data_quality"]["missing_values"][col] = missing_count
            
            # ì¤‘ë³µí–‰ ì²´í¬
            if 'timestamp' in df.columns and 'symbol' in df.columns:
                report["data_quality"]["duplicate_rows"] = df.duplicated(['timestamp', 'symbol']).sum()
            
            # ê°€ê²© ì´ìƒê°’ ì²´í¬ (ê°„ë‹¨í•œ ë²„ì „)
            price_cols = ['open', 'high', 'low', 'close']
            for col in price_cols:
                if col in df.columns:
                    # 0 ì´í•˜ ê°’
                    zero_or_negative = (df[col] <= 0).sum()
                    if zero_or_negative > 0:
                        report["data_quality"]["price_anomalies"].append(
                            f"{col}: {zero_or_negative} zero/negative values"
                        )
                    
                    # ê·¹ë‹¨ê°’ (ê°„ë‹¨í•œ ì²´í¬)
                    q99 = df[col].quantile(0.99)
                    q01 = df[col].quantile(0.01)
                    outliers = ((df[col] > q99 * 10) | (df[col] < q01 / 10)).sum()
                    if outliers > 0:
                        report["data_quality"]["price_anomalies"].append(
                            f"{col}: {outliers} potential outliers"
                        )
            
            # ì „ì²´ ìƒíƒœ ìš”ì•½
            issues = []
            issues.extend(report["schema_compliance"]["missing_columns"])
            issues.extend(list(report["data_quality"]["missing_values"].keys()))
            if report["data_quality"]["duplicate_rows"] > 0:
                issues.append("duplicates")
            if report["data_quality"]["price_anomalies"]:
                issues.append("price_anomalies")
            
            report["status"] = "clean" if not issues else "issues_found"
            report["issues_summary"] = issues
            
            return report
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Failed to analyze file: {str(e)}"
            }
    
    def save_processed_data(
        self,
        df: pd.DataFrame,
        filename: str,
        format: str = "parquet",
        validate: bool = True
    ):
        """
        ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ í¬í•¨)
        """
        if validate:
            df = ensure_candle_schema(df)
        
        file_path = self.data_dir / filename
        
        if format == "parquet":
            df.to_parquet(file_path, index=False)
        elif format == "csv":
            df.to_csv(file_path, index=False)
        elif format == "pickle":
            df.to_pickle(file_path)
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        print(f"Saved {len(df)} rows to {file_path}")


# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_load(filename: str, symbol: str = None) -> pd.DataFrame:
    """ë¹ ë¥¸ ë°ì´í„° ë¡œë”©"""
    loader = BacktestDataLoader()
    return loader.load_candles_from_file(filename, symbol_filter=symbol)


def data_summary(filename: str) -> None:
    """ë°ì´í„° íŒŒì¼ ìš”ì•½ ì¶œë ¥"""
    loader = BacktestDataLoader()
    report = loader.get_data_quality_report(filename)
    
    print(f"\nğŸ“Š Data Summary: {filename}")
    print("-" * 50)
    print(f"Status: {report['status']}")
    print(f"Total rows: {report['file_info']['total_rows']:,}")
    print(f"Memory usage: {report['file_info']['memory_usage_mb']:.1f} MB")
    
    if report['date_range']['start']:
        print(f"Date range: {report['date_range']['start']} to {report['date_range']['end']}")
        print(f"Duration: {report['date_range']['days']} days")
    
    print(f"Symbols: {len(report['symbols'])} ({', '.join(report['symbols'][:5])}{'...' if len(report['symbols']) > 5 else ''})")
    
    if report['issues_summary']:
        print(f"âš ï¸  Issues found: {', '.join(report['issues_summary'])}")
    else:
        print("âœ… No issues detected")


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    loader = BacktestDataLoader()
    available = loader.list_available_data()
    print("Available data files:", available)