"""
ğŸ“‚ ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë”

processed/ í´ë”ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ í›„ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì— ì „ë‹¬í•©ë‹ˆë‹¤.
ëª¨ë“  ë°ì´í„°ëŠ” í‘œì¤€ ìŠ¤í‚¤ë§ˆë¥¼ ê±°ì³ì•¼ ë°±í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Union, Tuple
import glob
import pickle

from data_tools.schema import (
    Candle, ensure_candle_schema, validate_candle_data, 
    candles_to_dataframe, dataframe_to_candles, REQUIRED_CANDLE_COLUMNS
)
from data_tools.build_datasets import DatasetBuilder


class BacktestDataLoader:
    """
    ğŸ”„ ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë”
    
    ì—­í• :
    1. processed/ í´ë”ì—ì„œ ë°ì´í„° íŒŒì¼ ì½ê¸°
    2. í‘œì¤€ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ê°•ì œ ì ìš©  
    3. ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì´ ì›í•˜ëŠ” í˜•íƒœë¡œ ë°ì´í„° ì œê³µ
    4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ë¡œë”© ì§€ì›
    """
    
    def __init__(self, data_dir: str = "backtest_data/processed"):
        self.data_dir = Path(data_dir)
        self.cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ
        self._ensure_data_dir()
    
    def _ensure_data_dir(self):
        """ë°ì´í„° ë””ë ‰í„°ë¦¬ ìƒì„±"""
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def list_available_data(self) -> Dict[str, List[str]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ëª©ë¡"""
        files = {}
        
        # CSV íŒŒì¼ë“¤
        csv_files = list(self.data_dir.glob("*.csv"))
        if csv_files:
            files['csv'] = [f.name for f in csv_files]
        
        # Parquet íŒŒì¼ë“¤  
        parquet_files = list(self.data_dir.glob("*.parquet"))
        if parquet_files:
            files['parquet'] = [f.name for f in parquet_files]
        
        # Pickle íŒŒì¼ë“¤
        pickle_files = list(self.data_dir.glob("*.pkl"))
        if pickle_files:
            files['pickle'] = [f.name for f in pickle_files]
        
        return files
    
    def load_candles_from_file(
        self, 
        filename: str,
        symbol_filter: Optional[str] = None,
        date_range: Optional[Tuple[datetime, datetime]] = None,
        validate: bool = True
    ) -> pd.DataFrame:
        """
        ğŸ“Š íŒŒì¼ì—ì„œ ìº”ë“¤ ë°ì´í„° ë¡œë“œ ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦
        
        Args:
            filename: ë°ì´í„° íŒŒì¼ëª…
            symbol_filter: íŠ¹ì • ì‹¬ë³¼ë§Œ í•„í„°ë§
            date_range: (ì‹œì‘ì¼, ì¢…ë£Œì¼) ë²”ìœ„ í•„í„°ë§  
            validate: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì—¬ë¶€
            
        Returns:
            ê²€ì¦ëœ ìº”ë“¤ DataFrame
        """
        file_path = self.data_dir / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"Data file not found: {file_path}")
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{filename}_{symbol_filter}_{date_range}"
        if cache_key in self.cache:
            return self.cache[cache_key].copy()
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë”©
        if filename.endswith('.csv'):
            df = pd.read_csv(file_path, parse_dates=['timestamp'])
        elif filename.endswith('.parquet'):
            df = pd.read_parquet(file_path)
        elif filename.endswith('.pkl'):
            df = pd.read_pickle(file_path)
        else:
            raise ValueError(f"Unsupported file format: {filename}")
        
        if df.empty:
            return df
        
        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ê°•ì œ
        if validate:
            try:
                df = ensure_candle_schema(df, strict=True)
            except Exception as e:
                raise ValueError(f"Schema validation failed for {filename}: {e}")
        
        # í•„í„°ë§ ì ìš©
        if symbol_filter:
            df = df[df['symbol'] == symbol_filter]
        
        if date_range:
            start_date, end_date = date_range
            df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]
        
        # ì‹œê°„ìˆœ ì •ë ¬
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤)
        if len(self.cache) < 10:  # ìµœëŒ€ 10ê°œ íŒŒì¼ ìºì‹œ
            self.cache[cache_key] = df.copy()
        
        return df
    
    def load_multiple_symbols(
        self,
        symbols: List[str],
        start_date: datetime,
        end_date: datetime,
        interval: str = "1h"
    ) -> Dict[str, pd.DataFrame]:
        """
        ğŸ“ˆ ì—¬ëŸ¬ ì‹¬ë³¼ì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ë¡œë“œ
        
        Returns:
            {symbol: DataFrame} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        result = {}
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤ í™•ì¸
        available_files = self.list_available_data()
        all_files = []
        for file_type, files in available_files.items():
            all_files.extend(files)
        
        for symbol in symbols:
            symbol_data = None
            
            # ì‹¬ë³¼ë³„ ì „ìš© íŒŒì¼ ì°¾ê¸°
            symbol_files = [f for f in all_files if symbol.replace('-', '_') in f]
            
            if symbol_files:
                # ê°€ì¥ ì ì ˆí•œ íŒŒì¼ ì„ íƒ (ê°„ê²© ë§¤ì¹­)
                best_file = None
                for file in symbol_files:
                    if interval in file:
                        best_file = file
                        break
                
                if not best_file:
                    best_file = symbol_files[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
                
                try:
                    symbol_data = self.load_candles_from_file(
                        best_file,
                        symbol_filter=symbol,
                        date_range=(start_date, end_date)
                    )
                except Exception as e:
                    print(f"Warning: Failed to load {symbol} from {best_file}: {e}")
            
            if symbol_data is None or symbol_data.empty:
                # í†µí•© íŒŒì¼ì—ì„œ ì°¾ê¸°
                for file in all_files:
                    try:
                        symbol_data = self.load_candles_from_file(
                            file,
                            symbol_filter=symbol,
                            date_range=(start_date, end_date)
                        )
                        if not symbol_data.empty:
                            break
                    except Exception:
                        continue
            
            result[symbol] = symbol_data if symbol_data is not None else pd.DataFrame()
        
        return result
    
    def create_batch_iterator(
        self,
        filename: str,
        batch_size_days: int = 30,
        overlap_days: int = 1,
        **kwargs
    ):
        """
        ğŸ”„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì´í„°ë ˆì´í„°
        
        í° ë°ì´í„°ì…‹ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
        """
        # ì „ì²´ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        df_sample = self.load_candles_from_file(filename, **kwargs)
        if df_sample.empty:
            return
        
        start_date = df_sample['timestamp'].min()
        end_date = df_sample['timestamp'].max()
        
        current_start = start_date
        batch_delta = timedelta(days=batch_size_days)
        overlap_delta = timedelta(days=overlap_days)
        
        while current_start < end_date:
            current_end = min(current_start + batch_delta, end_date)
            
            # ë°°ì¹˜ ë¡œë“œ
            batch_df = self.load_candles_from_file(
                filename,
                date_range=(current_start, current_end),
                **kwargs
            )
            
            if not batch_df.empty:
                yield batch_df
            
            # ë‹¤ìŒ ë°°ì¹˜ ì‹œì‘ì  (ì˜¤ë²„ë© ê³ ë ¤)
            current_start = current_end - overlap_delta
    
    def get_data_quality_report(self, filename: str) -> Dict:
        """
        ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
        """
        try:
            df = self.load_candles_from_file(filename, validate=False)
            
            if df.empty:
                return {"status": "empty", "message": "No data in file"}
            
            # ê¸°ë³¸ í†µê³„
            report = {
                "file_info": {
                    "filename": filename,
                    "total_rows": len(df),
                    "memory_usage_mb": df.memory_usage(deep=True).sum() / 1024 / 1024
                },
                "date_range": {
                    "start": df['timestamp'].min() if 'timestamp' in df.columns else None,
                    "end": df['timestamp'].max() if 'timestamp' in df.columns else None,
                    "days": None
                },
                "symbols": list(df['symbol'].unique()) if 'symbol' in df.columns else [],
                "schema_compliance": {
                    "missing_columns": [],
                    "extra_columns": [],
                    "type_issues": []
                },
                "data_quality": {
                    "missing_values": {},
                    "duplicate_rows": 0,
                    "price_anomalies": []
                }
            }
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            if report["date_range"]["start"] and report["date_range"]["end"]:
                report["date_range"]["days"] = (
                    report["date_range"]["end"] - report["date_range"]["start"]
                ).days
            
            # ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ ì²´í¬
            required_cols = set(REQUIRED_CANDLE_COLUMNS)
            actual_cols = set(df.columns)
            
            report["schema_compliance"]["missing_columns"] = list(required_cols - actual_cols)
            report["schema_compliance"]["extra_columns"] = list(actual_cols - required_cols)
            
            # ê²°ì¸¡ê°’ ì²´í¬
            for col in df.columns:
                missing_count = df[col].isna().sum()
                if missing_count > 0:
                    report["data_quality"]["missing_values"][col] = missing_count
            
            # ì¤‘ë³µí–‰ ì²´í¬
            if 'timestamp' in df.columns and 'symbol' in df.columns:
                report["data_quality"]["duplicate_rows"] = df.duplicated(['timestamp', 'symbol']).sum()
            
            # ê°€ê²© ì´ìƒê°’ ì²´í¬ (ê°„ë‹¨í•œ ë²„ì „)
            price_cols = ['open', 'high', 'low', 'close']
            for col in price_cols:
                if col in df.columns:
                    # 0 ì´í•˜ ê°’
                    zero_or_negative = (df[col] <= 0).sum()
                    if zero_or_negative > 0:
                        report["data_quality"]["price_anomalies"].append(
                            f"{col}: {zero_or_negative} zero/negative values"
                        )
                    
                    # ê·¹ë‹¨ê°’ (ê°„ë‹¨í•œ ì²´í¬)
                    q99 = df[col].quantile(0.99)
                    q01 = df[col].quantile(0.01)
                    outliers = ((df[col] > q99 * 10) | (df[col] < q01 / 10)).sum()
                    if outliers > 0:
                        report["data_quality"]["price_anomalies"].append(
                            f"{col}: {outliers} potential outliers"
                        )
            
            # ì „ì²´ ìƒíƒœ ìš”ì•½
            issues = []
            issues.extend(report["schema_compliance"]["missing_columns"])
            issues.extend(list(report["data_quality"]["missing_values"].keys()))
            if report["data_quality"]["duplicate_rows"] > 0:
                issues.append("duplicates")
            if report["data_quality"]["price_anomalies"]:
                issues.append("price_anomalies")
            
            report["status"] = "clean" if not issues else "issues_found"
            report["issues_summary"] = issues
            
            return report
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Failed to analyze file: {str(e)}"
            }
    
    def save_processed_data(
        self,
        df: pd.DataFrame,
        filename: str,
        format: str = "parquet",
        validate: bool = True
    ):
        """
        ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ í¬í•¨)
        """
        if validate:
            df = ensure_candle_schema(df)
        
        file_path = self.data_dir / filename
        
        if format == "parquet":
            df.to_parquet(file_path, index=False)
        elif format == "csv":
            df.to_csv(file_path, index=False)
        elif format == "pickle":
            df.to_pickle(file_path)
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        print(f"Saved {len(df)} rows to {file_path}")
    
    def _generate_filename(
        self,
        symbol: str,
        interval: str,
        start_date: datetime,
        end_date: datetime
    ) -> str:
        """
        íŒŒì¼ëª… ìƒì„± (build_datasets.pyì™€ ë™ì¼í•œ íŒ¨í„´)
        
        Args:
            symbol: ì‹¬ë³¼
            interval: ê°„ê²©
            start_date: ì‹œì‘ì¼
            end_date: ì¢…ë£Œì¼
            
        Returns:
            íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
        """
        norm_symbol = symbol.replace('-', '_').replace('/', '_').lower()
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        return f"{norm_symbol}_{interval}_{start_str}_{end_str}_processed"
    
    def _find_data_file(
        self,
        symbol: str,
        interval: str,
        start_date: datetime,
        end_date: datetime
    ) -> Optional[str]:
        """
        ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        
        Args:
            symbol: ì‹¬ë³¼
            interval: ê°„ê²©
            start_date: ì‹œì‘ì¼
            end_date: ì¢…ë£Œì¼
            
        Returns:
            íŒŒì¼ëª… (ì—†ìœ¼ë©´ None)
        """
        expected_filename_base = self._generate_filename(symbol, interval, start_date, end_date)
        
        # CSV, Parquet íŒŒì¼ í™•ì¸
        for ext in ['.csv', '.parquet']:
            filename = expected_filename_base + ext
            file_path = self.data_dir / filename
            if file_path.exists():
                return filename
        
        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ë‚ ì§œ ë²”ìœ„ê°€ í¬í•¨ë˜ëŠ” íŒŒì¼ ì°¾ê¸°)
        available_files = self.list_available_data()
        all_files = []
        for file_type, files in available_files.items():
            all_files.extend(files)
        
        # ì‹¬ë³¼ê³¼ ê°„ê²©ì´ ì¼ì¹˜í•˜ëŠ” íŒŒì¼ ì°¾ê¸°
        norm_symbol = symbol.replace('-', '_').lower()
        for filename in all_files:
            if norm_symbol in filename.lower() and interval in filename:
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                try:
                    # íŒŒì¼ ë¡œë“œí•´ì„œ ë‚ ì§œ ë²”ìœ„ í™•ì¸
                    df = self.load_candles_from_file(filename, symbol_filter=symbol, validate=False)
                    if not df.empty:
                        file_start = df['timestamp'].min()
                        file_end = df['timestamp'].max()
                        # ìš”ì²­í•œ ë‚ ì§œ ë²”ìœ„ê°€ íŒŒì¼ ë²”ìœ„ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                        if file_start <= start_date and file_end >= end_date:
                            return filename
                except Exception:
                    continue
        
        return None
    
    def load_data_for_backtest(
        self,
        symbol: str,
        interval: str,
        days: int,
        exchange: str = "upbit"
    ) -> pd.DataFrame:
        """
        ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        
        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: "KRW-BTC")
            interval: ìº”ë“¤ ê°„ê²© (ì˜ˆ: "1h", "1d")
            days: ë©°ì¹ ì¹˜ ë°ì´í„° (í˜„ì¬ë¶€í„° Nì¼ ì „ê¹Œì§€)
            exchange: ê±°ë˜ì†Œ (ê¸°ë³¸ê°’: "upbit")
            
        Returns:
            ë°ì´í„° DataFrame
            
        Raises:
            FileNotFoundError: ë°ì´í„° ìƒì„± ì‹¤íŒ¨ ì‹œ
            ValueError: ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ
        """
        import pytz
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        end_date = datetime.now(pytz.UTC)
        start_date = end_date - timedelta(days=days)
        
        print(f"   Request: {symbol} {interval}, {days} days ({start_date.date()} ~ {end_date.date()})")
        
        # 1. ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        filename = self._find_data_file(symbol, interval, start_date, end_date)
        
        if filename:
            print(f"   [OK] Existing data file found: {filename}")
            try:
                df = self.load_candles_from_file(
                    filename,
                    symbol_filter=symbol,
                    date_range=(start_date, end_date),
                    validate=True
                )
                if not df.empty:
                    print(f"   [OK] Data loaded: {len(df)} candles")
                    return df
            except Exception as e:
                print(f"   [WARN] Failed to load existing file: {e}")
                print(f"   Generating new data...")
        
        # 2. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒì„±
        print(f"   [INFO] Collecting data...")
        builder = DatasetBuilder()
        
        try:
            result = builder.build_single_dataset(
                exchange=exchange,
                symbol=symbol,
                interval=interval,
                start_date=start_date,
                end_date=end_date,
                save_formats=["csv"]  # CSVë§Œ ì €ì¥ (ë¹ ë¥¸ ë¡œë”©)
            )
            
            if result["status"] != "completed":
                error_msg = result.get("errors", ["Unknown error"])
                raise FileNotFoundError(f"ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {', '.join(error_msg)}")
            
            # 3. ìƒì„±ëœ íŒŒì¼ ì½ê¸°
            created_files = result.get("files_created", [])
            csv_files = [f for f in created_files if f.endswith('.csv')]
            
            if not csv_files:
                raise FileNotFoundError("ìƒì„±ëœ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ì „ì²´ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ)
            created_file_path = Path(csv_files[0])
            created_filename = created_file_path.name
            
            print(f"   [OK] Data generated: {created_filename}")
            
            # ìƒì„±ëœ íŒŒì¼ ì½ê¸° (ë‚ ì§œ í•„í„°ë§ ì—†ì´ ì „ì²´ ë¡œë“œ í›„ í•„í„°ë§)
            try:
                # symbol_filter ì—†ì´ ë¨¼ì € ë¡œë“œ (ìŠ¤í‚¤ë§ˆ í™•ì¸ìš©)
                df = self.load_candles_from_file(
                    created_filename,
                    symbol_filter=None,  # í•„í„° ì—†ì´ ì „ì²´ ë¡œë“œ
                    date_range=None,  # ì „ì²´ ë¡œë“œ
                    validate=True
                )
                
                # ì‹¬ë³¼ í•„í„°ë§ (ë¡œë“œ í›„) - í•„ìš”ì‹œì—ë§Œ
                if symbol and 'symbol' in df.columns and not df.empty:
                    unique_symbols = df['symbol'].unique()
                    if symbol in unique_symbols:
                        df = df[df['symbol'] == symbol]
                    # ì‹¬ë³¼ì´ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ì „ì²´ ë°ì´í„° ì‚¬ìš© (ë‹¨ì¼ ì‹¬ë³¼ íŒŒì¼ì´ë¯€ë¡œ)
            except Exception as load_error:
                raise ValueError(f"File load failed: {load_error}") from load_error
            
            if df.empty:
                raise ValueError("Generated data file is empty")
            
            # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ (ë¡œë“œ í›„)
            if 'timestamp' in df.columns:
                df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]
                df = df.sort_values('timestamp').reset_index(drop=True)
            
            if df.empty:
                raise ValueError(f"No data in requested date range ({start_date.date()} ~ {end_date.date()})")
            
            print(f"   [OK] Data loaded: {len(df)} candles")
            return df
            
        except Exception as e:
            error_msg = f"Data load failed: {str(e)}"
            print(f"   [ERROR] {error_msg}")
            raise FileNotFoundError(error_msg) from e


# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_load(filename: str, symbol: str = None) -> pd.DataFrame:
    """ë¹ ë¥¸ ë°ì´í„° ë¡œë”©"""
    loader = BacktestDataLoader()
    return loader.load_candles_from_file(filename, symbol_filter=symbol)


def data_summary(filename: str) -> None:
    """ë°ì´í„° íŒŒì¼ ìš”ì•½ ì¶œë ¥"""
    loader = BacktestDataLoader()
    report = loader.get_data_quality_report(filename)
    
    print(f"\nğŸ“Š Data Summary: {filename}")
    print("-" * 50)
    print(f"Status: {report['status']}")
    print(f"Total rows: {report['file_info']['total_rows']:,}")
    print(f"Memory usage: {report['file_info']['memory_usage_mb']:.1f} MB")
    
    if report['date_range']['start']:
        print(f"Date range: {report['date_range']['start']} to {report['date_range']['end']}")
        print(f"Duration: {report['date_range']['days']} days")
    
    print(f"Symbols: {len(report['symbols'])} ({', '.join(report['symbols'][:5])}{'...' if len(report['symbols']) > 5 else ''})")
    
    if report['issues_summary']:
        print(f"âš ï¸  Issues found: {', '.join(report['issues_summary'])}")
    else:
        print("âœ… No issues detected")


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    loader = BacktestDataLoader()
    available = loader.list_available_data()
    print("Available data files:", available)